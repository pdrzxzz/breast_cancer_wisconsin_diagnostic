# Breast Cancer Wisconsin Diagnostic Classification Project ğŸ—ï¸

## Overview ğŸ”
This project implements a comprehensive machine learning workflow for classifying breast cancer tumors as malignant or benign using the Wisconsin Diagnostic Dataset. The solution compares multiple classification algorithms with advanced hyperparameter tuning, detailed visualizations, and robust evaluation techniques.

![Classifier Comparison](https://github.com/pdrzxzz/breast_cancer_wisconsin_diagnostic/blob/main/images/models_optimized/optimized-evaluation.png)

## Key Features âœ¨

### Advanced Preprocessing Pipeline âš™ï¸
- **PowerTransformer**: Applied Yeo-Johnson transformation to normalize feature distributions ğŸ“Š
- **StandardScaler**: Standardized features to zero mean and unit variance âš–ï¸
- **Automated Pipeline**: Integrated transformations into a single preprocessing workflow ğŸ”„
- **Outlier Handling**: IQR-based outlier detection and removal with configurable thresholds ğŸš¨

### Hyperparameter Optimization ğŸ›ï¸
- **Recall-Focused Tuning**: Used `make_scorer` to prioritize sensitivity (recall) for malignant cases ğŸ¯
- **GridSearchCV**: Exhaustive search for Decision Trees and KNN ğŸ”
- **RandomizedSearchCV**: Efficient hyperparameter sampling for Logistic Regression ğŸ²
- **Custom Parameter Spaces**: Log-uniform distributions for regularization strength ğŸ“ˆ

### Advanced Model Evaluation ğŸ“Š
- **StratifiedShuffleSplit**: For reliable learning curve generation ğŸ“ˆ
- **ROC Curves**: Visual comparison of classifier performance with AUC metrics ğŸ“‰
- **Learning Curves**: Track precision, recall and F1 across training sizes ğŸ‹ï¸
- **Enhanced Confusion Matrices**: TN/FP/FN/TP visualization with absolute counts and percentages ğŸ”¢

### Comprehensive Visualizations ğŸš
- **Correlation Analysis**: Feature-target relationships with color-coded bar plots ğŸŒˆ
- **Box Plots**: Distribution analysis and outlier detection ğŸ“¦
- **Histograms**: Feature distribution visualization ğŸ“Š
- **Classification Heatmaps**: Styled presentation of precision, recall, and F1-scores ğŸ”¥

## Methodology ğŸ§ª

### Data Preparation ğŸ§¹
- Fetched dataset using `ucimlrepo` package ğŸ“¦
- Stratified 70-30 train-test split preserving class distribution âœ‚ï¸
- Removed low-correlation features (`symmetry2`, `fractal_dimension1`) ğŸ—‘ï¸

### Model Comparison ğŸ¤–
Evaluated four base classifiers:
1. Decision Trees ğŸŒ³
2. Logistic Regression ğŸ“‰
3. K-Nearest Neighbors ğŸ‘¥
4. Naive Bayes ğŸ“

### Optimization Techniques âš¡
- **Decision Trees**: Tuned max_depth, min_samples_leaf, and splitting criteria ğŸšï¸
- **Logistic Regression**: Optimized regularization strength (C) and penalty type (L1/L2) âš–ï¸
- **KNN**: Adjusted neighbors count, weighting, and distance metrics ğŸ“
- **Naive Bayes**: Calibrated var_smoothing parameter ğŸšï¸

### Evaluation Metrics ğŸ“
- Classification reports (precision, recall, F1-score) ğŸ“
- Confusion matrices with detailed annotations ğŸ”
- AUC-ROC curves ğŸ“ˆ
- Macro-averaged learning curves ğŸ“Š

## Results ğŸ“Œ
The visualization system generates comparative reports including:
- Side-by-side classifier performance heatmaps ğŸ”¥
- ROC curve comparisons ğŸ“‰
- Training size vs. metric learning curves ğŸ‹ï¸
- Before/after transformation distribution comparisons ğŸ”„
- Feature correlation rankings ğŸ†

## Technologies Used ğŸ’»
- **Python** (3.10+) ğŸ
- **scikit-learn** (Pipelines, Transformers, Models, Metrics) ğŸ¤–
- **pandas & numpy** (Data Manipulation) ğŸ§®
- **matplotlib & seaborn** (Visualizations) ğŸ¨
- **SciPy** (Statistical Functions) ğŸ“Š
- **ucimlrepo** (Dataset Access) ğŸ“‚

## Key Advanced Techniques ğŸš€
1. `PowerTransformer` for distribution normalization âš¡
2. `StratifiedShuffleSplit` for learning curve generation ğŸ“ˆ
3. `make_scorer` with custom recall focus ğŸ¯
4. Pipeline-based preprocessing workflow ğŸ”„
5. Multi-metric learning curves with error bars ğŸ“Š
6. Annotated confusion matrices ğŸ”
7. Correlation analysis with target mapping ğŸ—ºï¸
8. IQR-based outlier detection system ğŸš¨
